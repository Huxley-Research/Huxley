"""
Database schema and migration utilities for Huxley.

Supports:
- Supabase (PostgreSQL + pgvector)
- Neon (PostgreSQL + pgvector)
- Standard PostgreSQL
- SQLite (local development)
"""

import asyncio
from typing import Any


# ============================================================================
# BASE SCHEMA (Core Tables)
# ============================================================================

BASE_SCHEMA_POSTGRES = """
-- Huxley Core Schema
-- Generated by Huxley Memory Migrations

-- Enable UUID extension if not exists
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- Conversations table
CREATE TABLE IF NOT EXISTS huxley_conversations (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    session_id VARCHAR(64) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    metadata JSONB DEFAULT '{}'::jsonb,
    
    CONSTRAINT unique_session UNIQUE (session_id)
);

-- Messages table
CREATE TABLE IF NOT EXISTS huxley_messages (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    conversation_id UUID REFERENCES huxley_conversations(id) ON DELETE CASCADE,
    role VARCHAR(32) NOT NULL CHECK (role IN ('user', 'assistant', 'system', 'tool')),
    content TEXT NOT NULL,
    tool_calls JSONB,
    tool_results JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    metadata JSONB DEFAULT '{}'::jsonb
);

-- Memory store (key-value with expiry)
CREATE TABLE IF NOT EXISTS huxley_memory (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    key VARCHAR(512) NOT NULL UNIQUE,
    value JSONB NOT NULL,
    expires_at TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Research sessions
CREATE TABLE IF NOT EXISTS huxley_research_sessions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    session_id VARCHAR(64) NOT NULL UNIQUE,
    objective TEXT NOT NULL,
    status VARCHAR(32) DEFAULT 'running' CHECK (status IN ('running', 'completed', 'failed', 'cancelled')),
    iterations INTEGER DEFAULT 0,
    findings JSONB DEFAULT '[]'::jsonb,
    hypotheses JSONB DEFAULT '[]'::jsonb,
    viable_solutions JSONB DEFAULT '[]'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    completed_at TIMESTAMP WITH TIME ZONE,
    metadata JSONB DEFAULT '{}'::jsonb
);

-- Tool executions log
CREATE TABLE IF NOT EXISTS huxley_tool_executions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    session_id VARCHAR(64),
    tool_name VARCHAR(128) NOT NULL,
    parameters JSONB,
    result JSONB,
    success BOOLEAN DEFAULT true,
    duration_ms INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Molecules designed by Huxley
CREATE TABLE IF NOT EXISTS huxley_molecules (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    molecule_id VARCHAR(64) NOT NULL UNIQUE,
    smiles TEXT NOT NULL,
    name VARCHAR(256),
    target VARCHAR(64),
    properties JSONB DEFAULT '{}'::jsonb,
    druglikeness JSONB DEFAULT '{}'::jsonb,
    docking_results JSONB DEFAULT '[]'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    session_id VARCHAR(64),
    metadata JSONB DEFAULT '{}'::jsonb
);

-- Create indexes
CREATE INDEX IF NOT EXISTS idx_messages_conversation ON huxley_messages(conversation_id);
CREATE INDEX IF NOT EXISTS idx_messages_created ON huxley_messages(created_at);
CREATE INDEX IF NOT EXISTS idx_memory_key ON huxley_memory(key);
CREATE INDEX IF NOT EXISTS idx_memory_expires ON huxley_memory(expires_at) WHERE expires_at IS NOT NULL;
CREATE INDEX IF NOT EXISTS idx_research_session ON huxley_research_sessions(session_id);
CREATE INDEX IF NOT EXISTS idx_tool_session ON huxley_tool_executions(session_id);
CREATE INDEX IF NOT EXISTS idx_molecules_target ON huxley_molecules(target);
CREATE INDEX IF NOT EXISTS idx_molecules_session ON huxley_molecules(session_id);

-- Auto-update updated_at trigger
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

DROP TRIGGER IF EXISTS update_conversations_updated_at ON huxley_conversations;
CREATE TRIGGER update_conversations_updated_at
    BEFORE UPDATE ON huxley_conversations
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

DROP TRIGGER IF EXISTS update_memory_updated_at ON huxley_memory;
CREATE TRIGGER update_memory_updated_at
    BEFORE UPDATE ON huxley_memory
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
"""

BASE_SCHEMA_SQLITE = """
-- Huxley Core Schema (SQLite)

-- Conversations table
CREATE TABLE IF NOT EXISTS huxley_conversations (
    id TEXT PRIMARY KEY,
    session_id TEXT NOT NULL UNIQUE,
    created_at TEXT DEFAULT (datetime('now')),
    updated_at TEXT DEFAULT (datetime('now')),
    metadata TEXT DEFAULT '{}'
);

-- Messages table
CREATE TABLE IF NOT EXISTS huxley_messages (
    id TEXT PRIMARY KEY,
    conversation_id TEXT REFERENCES huxley_conversations(id) ON DELETE CASCADE,
    role TEXT NOT NULL CHECK (role IN ('user', 'assistant', 'system', 'tool')),
    content TEXT NOT NULL,
    tool_calls TEXT,
    tool_results TEXT,
    created_at TEXT DEFAULT (datetime('now')),
    metadata TEXT DEFAULT '{}'
);

-- Memory store
CREATE TABLE IF NOT EXISTS huxley_memory (
    id TEXT PRIMARY KEY,
    key TEXT NOT NULL UNIQUE,
    value TEXT NOT NULL,
    expires_at TEXT,
    created_at TEXT DEFAULT (datetime('now')),
    updated_at TEXT DEFAULT (datetime('now'))
);

-- Research sessions
CREATE TABLE IF NOT EXISTS huxley_research_sessions (
    id TEXT PRIMARY KEY,
    session_id TEXT NOT NULL UNIQUE,
    objective TEXT NOT NULL,
    status TEXT DEFAULT 'running' CHECK (status IN ('running', 'completed', 'failed', 'cancelled')),
    iterations INTEGER DEFAULT 0,
    findings TEXT DEFAULT '[]',
    hypotheses TEXT DEFAULT '[]',
    viable_solutions TEXT DEFAULT '[]',
    created_at TEXT DEFAULT (datetime('now')),
    completed_at TEXT,
    metadata TEXT DEFAULT '{}'
);

-- Tool executions
CREATE TABLE IF NOT EXISTS huxley_tool_executions (
    id TEXT PRIMARY KEY,
    session_id TEXT,
    tool_name TEXT NOT NULL,
    parameters TEXT,
    result TEXT,
    success INTEGER DEFAULT 1,
    duration_ms INTEGER,
    created_at TEXT DEFAULT (datetime('now'))
);

-- Molecules
CREATE TABLE IF NOT EXISTS huxley_molecules (
    id TEXT PRIMARY KEY,
    molecule_id TEXT NOT NULL UNIQUE,
    smiles TEXT NOT NULL,
    name TEXT,
    target TEXT,
    properties TEXT DEFAULT '{}',
    druglikeness TEXT DEFAULT '{}',
    docking_results TEXT DEFAULT '[]',
    created_at TEXT DEFAULT (datetime('now')),
    session_id TEXT,
    metadata TEXT DEFAULT '{}'
);

-- Indexes
CREATE INDEX IF NOT EXISTS idx_messages_conversation ON huxley_messages(conversation_id);
CREATE INDEX IF NOT EXISTS idx_memory_key ON huxley_memory(key);
CREATE INDEX IF NOT EXISTS idx_research_session ON huxley_research_sessions(session_id);
"""


# ============================================================================
# VECTOR SCHEMA (AI Memory with Embeddings)
# ============================================================================

VECTOR_SCHEMA_PGVECTOR = """
-- Huxley Vector Schema (pgvector)
-- For semantic search and AI memory

-- Enable pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Vector embeddings table
CREATE TABLE IF NOT EXISTS huxley_embeddings (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    content TEXT NOT NULL,
    embedding vector(1536),  -- OpenAI ada-002 dimension
    content_type VARCHAR(64) DEFAULT 'text',
    source VARCHAR(256),
    session_id VARCHAR(64),
    metadata JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Long-term AI memory
CREATE TABLE IF NOT EXISTS huxley_ai_memory (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    memory_type VARCHAR(64) NOT NULL CHECK (memory_type IN ('fact', 'preference', 'context', 'tool_result', 'research_finding')),
    content TEXT NOT NULL,
    embedding vector(1536),
    importance FLOAT DEFAULT 0.5,
    access_count INTEGER DEFAULT 0,
    last_accessed TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    session_id VARCHAR(64),
    metadata JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Document chunks for RAG
CREATE TABLE IF NOT EXISTS huxley_documents (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id VARCHAR(256) NOT NULL,
    chunk_index INTEGER NOT NULL,
    content TEXT NOT NULL,
    embedding vector(1536),
    document_type VARCHAR(64),
    source_url TEXT,
    metadata JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    CONSTRAINT unique_chunk UNIQUE (document_id, chunk_index)
);

-- Create vector indexes for similarity search
-- Using ivfflat for better performance on large datasets
CREATE INDEX IF NOT EXISTS idx_embeddings_vector ON huxley_embeddings 
    USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

CREATE INDEX IF NOT EXISTS idx_ai_memory_vector ON huxley_ai_memory 
    USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

CREATE INDEX IF NOT EXISTS idx_documents_vector ON huxley_documents 
    USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

-- Regular indexes
CREATE INDEX IF NOT EXISTS idx_embeddings_session ON huxley_embeddings(session_id);
CREATE INDEX IF NOT EXISTS idx_ai_memory_type ON huxley_ai_memory(memory_type);
CREATE INDEX IF NOT EXISTS idx_ai_memory_importance ON huxley_ai_memory(importance DESC);
CREATE INDEX IF NOT EXISTS idx_documents_doc_id ON huxley_documents(document_id);

-- Function for similarity search
CREATE OR REPLACE FUNCTION huxley_search_similar(
    query_embedding vector(1536),
    match_threshold FLOAT DEFAULT 0.7,
    match_count INT DEFAULT 10
)
RETURNS TABLE (
    id UUID,
    content TEXT,
    similarity FLOAT,
    metadata JSONB
)
LANGUAGE plpgsql
AS $$
BEGIN
    RETURN QUERY
    SELECT 
        e.id,
        e.content,
        1 - (e.embedding <=> query_embedding) as similarity,
        e.metadata
    FROM huxley_embeddings e
    WHERE 1 - (e.embedding <=> query_embedding) > match_threshold
    ORDER BY e.embedding <=> query_embedding
    LIMIT match_count;
END;
$$;

-- Function to search AI memory
CREATE OR REPLACE FUNCTION huxley_recall_memory(
    query_embedding vector(1536),
    memory_types TEXT[] DEFAULT ARRAY['fact', 'preference', 'context', 'tool_result', 'research_finding'],
    match_count INT DEFAULT 5
)
RETURNS TABLE (
    id UUID,
    memory_type VARCHAR(64),
    content TEXT,
    importance FLOAT,
    similarity FLOAT,
    metadata JSONB
)
LANGUAGE plpgsql
AS $$
BEGIN
    RETURN QUERY
    SELECT 
        m.id,
        m.memory_type,
        m.content,
        m.importance,
        1 - (m.embedding <=> query_embedding) as similarity,
        m.metadata
    FROM huxley_ai_memory m
    WHERE m.memory_type = ANY(memory_types)
    ORDER BY 
        (1 - (m.embedding <=> query_embedding)) * m.importance DESC
    LIMIT match_count;
    
    -- Update access count for retrieved memories
    UPDATE huxley_ai_memory 
    SET access_count = access_count + 1,
        last_accessed = NOW()
    WHERE huxley_ai_memory.id IN (
        SELECT m2.id 
        FROM huxley_ai_memory m2
        WHERE m2.memory_type = ANY(memory_types)
        ORDER BY (1 - (m2.embedding <=> query_embedding)) * m2.importance DESC
        LIMIT match_count
    );
END;
$$;
"""


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def get_base_schema(provider: str = "postgresql") -> str:
    """Get base schema SQL for the specified provider."""
    if provider.lower() in ("sqlite", "sqlite3"):
        return BASE_SCHEMA_SQLITE
    return BASE_SCHEMA_POSTGRES


def get_vector_schema(provider: str = "supabase") -> str:
    """Get vector schema SQL for providers that support pgvector."""
    # Supabase and Neon both support pgvector
    if provider.lower() in ("supabase", "neon", "postgresql"):
        return VECTOR_SCHEMA_PGVECTOR
    return ""


async def test_connection(url: str) -> dict[str, Any]:
    """
    Test database connection and detect capabilities.
    
    Returns:
        Dict with connection status and capabilities
    """
    result = {
        "connected": False,
        "provider": "unknown",
        "supports_vectors": False,
        "error": None,
    }
    
    try:
        if url.startswith("sqlite"):
            import aiosqlite
            
            db_path = url.replace("sqlite:///", "").replace("sqlite://", "")
            async with aiosqlite.connect(db_path) as db:
                await db.execute("SELECT 1")
                result["connected"] = True
                result["provider"] = "sqlite"
                result["supports_vectors"] = False
                
        elif url.startswith("postgresql") or url.startswith("postgres"):
            import asyncpg
            
            conn = await asyncpg.connect(url)
            try:
                await conn.execute("SELECT 1")
                result["connected"] = True
                
                # Detect provider from URL
                url_lower = url.lower()
                if "supabase" in url_lower:
                    result["provider"] = "supabase"
                    result["supports_vectors"] = True
                elif "neon" in url_lower:
                    result["provider"] = "neon"
                    result["supports_vectors"] = True
                else:
                    result["provider"] = "postgresql"
                    # Check if pgvector is available
                    try:
                        await conn.execute("SELECT 'vector'::regtype")
                        result["supports_vectors"] = True
                    except:
                        result["supports_vectors"] = False
            finally:
                await conn.close()
                
    except ImportError as e:
        result["error"] = f"Missing dependency: {e}. Install with: pip install asyncpg aiosqlite"
    except Exception as e:
        result["error"] = str(e)
    
    return result


async def setup_database(url: str, include_vectors: bool = True) -> dict[str, Any]:
    """
    Set up database tables.
    
    Args:
        url: Database connection URL
        include_vectors: Whether to set up vector tables (if supported)
        
    Returns:
        Dict with setup status
    """
    result = {
        "success": False,
        "tables_created": [],
        "vectors_enabled": False,
        "error": None,
    }
    
    try:
        # Test connection first
        conn_result = await test_connection(url)
        if not conn_result["connected"]:
            result["error"] = conn_result.get("error", "Failed to connect")
            return result
        
        provider = conn_result["provider"]
        
        if provider == "sqlite":
            import aiosqlite
            
            db_path = url.replace("sqlite:///", "").replace("sqlite://", "")
            async with aiosqlite.connect(db_path) as db:
                # Execute base schema
                for statement in BASE_SCHEMA_SQLITE.split(";"):
                    statement = statement.strip()
                    if statement:
                        await db.execute(statement)
                await db.commit()
                
            result["success"] = True
            result["tables_created"] = [
                "huxley_conversations",
                "huxley_messages",
                "huxley_memory",
                "huxley_research_sessions",
                "huxley_tool_executions",
                "huxley_molecules",
            ]
            
        else:  # PostgreSQL-based
            import asyncpg
            
            conn = await asyncpg.connect(url)
            try:
                # Execute base schema
                await conn.execute(BASE_SCHEMA_POSTGRES)
                result["tables_created"] = [
                    "huxley_conversations",
                    "huxley_messages", 
                    "huxley_memory",
                    "huxley_research_sessions",
                    "huxley_tool_executions",
                    "huxley_molecules",
                ]
                
                # Set up vectors if supported and requested
                if include_vectors and conn_result["supports_vectors"]:
                    try:
                        await conn.execute(VECTOR_SCHEMA_PGVECTOR)
                        result["vectors_enabled"] = True
                        result["tables_created"].extend([
                            "huxley_embeddings",
                            "huxley_ai_memory",
                            "huxley_documents",
                        ])
                    except Exception as e:
                        # Vector setup failed but base tables are OK
                        result["vector_error"] = str(e)
                        
                result["success"] = True
                
            finally:
                await conn.close()
                
    except ImportError as e:
        result["error"] = f"Missing dependency: {e}"
    except Exception as e:
        result["error"] = str(e)
    
    return result


async def setup_vectors(url: str) -> dict[str, Any]:
    """
    Set up only vector tables (for existing databases).
    
    Args:
        url: Database connection URL
        
    Returns:
        Dict with setup status
    """
    result = {
        "success": False,
        "tables_created": [],
        "error": None,
    }
    
    try:
        conn_result = await test_connection(url)
        if not conn_result["connected"]:
            result["error"] = conn_result.get("error", "Failed to connect")
            return result
            
        if not conn_result["supports_vectors"]:
            result["error"] = f"Provider {conn_result['provider']} does not support pgvector"
            return result
        
        import asyncpg
        
        conn = await asyncpg.connect(url)
        try:
            await conn.execute(VECTOR_SCHEMA_PGVECTOR)
            result["success"] = True
            result["tables_created"] = [
                "huxley_embeddings",
                "huxley_ai_memory",
                "huxley_documents",
            ]
        finally:
            await conn.close()
            
    except Exception as e:
        result["error"] = str(e)
    
    return result


# Sync wrappers for CLI use
def setup_database_sync(url: str, include_vectors: bool = True) -> dict[str, Any]:
    """Synchronous wrapper for setup_database."""
    return asyncio.get_event_loop().run_until_complete(
        setup_database(url, include_vectors)
    )


def test_connection_sync(url: str) -> dict[str, Any]:
    """Synchronous wrapper for test_connection."""
    return asyncio.get_event_loop().run_until_complete(test_connection(url))
